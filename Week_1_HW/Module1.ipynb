{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d2eb5fbe70bea445a5bdf28ac0697b",
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50ce8f8109224a024605fa44f6a5958b",
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4571ea879ea92d09a2831fcdac74887",
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c4c380877a398b95cc910c3e3f0244",
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a709a717c9fd199ede5881a38c151b97",
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        scaler = StandardScaler()\n",
    "        transformed_features = scaler.fit_transform(X)\n",
    "        return(transformed_features)\n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        # I am currently operating under the assumption that the means we are taking are of all observations over \n",
    "        ## a given feature, for all features. That would result in n computations, which can be stored in n X 1 vector.\n",
    "        \n",
    "        means = np.mean(X_std, axis = 0)\n",
    "        return(means)\n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0] - 1)\n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        (eigenvals, eigenvecs) = np.linalg.eig(cov_mat)\n",
    "        return (eigenvals, eigenvecs)\n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        #I had to reupload because this question among others got misgraded. This function successfully sorts the\n",
    "        ## eigenvals ahd calculates the explained variance by each sequentially.\n",
    "        \n",
    "        eigen_vals[::-1].sort()\n",
    "        total_var = eigen_vals.sum()\n",
    "        explained_var = (1 / total_var) * eigen_vals\n",
    "        return(explained_var)\n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "    \n",
    "    def make_eig_pairs(self, eigenvals, eigenvecs):\n",
    "        \"\"\"\n",
    "        Citation: https://stackoverflow.com/questions/9007877/sort-arrays-rows-by-another-array-in-python\n",
    "        Input: np.ndarrays eigenvals, eigenvecs. These are outputted by compute_eigen_vector\n",
    "        Output: list of doubles (eigenvalue, eigenvector) sorted by decreasing eigenvalue\n",
    "        \"\"\"\n",
    "        indecies = eigenvals.argsort()\n",
    "        sorted_eigenvals = eigenvals[indecies[::-1]]\n",
    "        sorted_eigenvecs = eigenvecs[indecies[::-1], :]\n",
    "        n = sorted_eigenvals.shape[0]\n",
    "        eig_pairs = list()\n",
    "        \n",
    "        for j in range(n):\n",
    "            this_double = (sorted_eigenvals[j], sorted_eigenvecs[j, :])\n",
    "            eig_pairs.append(this_double)\n",
    "        \n",
    "        return(eig_pairs)\n",
    "            \n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        my_target = self.target_explained_variance\n",
    "        # I'm just going to scan over cum_var_exp, which should run in O(k). This could be sped up using binary search\n",
    "        ## because cum_var_exp is already sorted. That would run in O(log(k)).\n",
    "        \n",
    "        k = 0\n",
    "        n = cum_var_exp.shape[0]\n",
    "        for feature in range(n):\n",
    "            if cum_var_exp[feature] >= my_target:\n",
    "                break\n",
    "            else:\n",
    "                k = k + 1\n",
    "        \n",
    "        # Now that I have the index of the last desired component, we just mash the corresponding eigenvectors into a matrix\n",
    "        eigenvec_container = list()\n",
    "        for feature in range(k):\n",
    "            this_eigenvec = eig_pairs[feature][1]\n",
    "            eigenvec_container.append(this_eigenvec)\n",
    "        eigenvecs = np.asarray(eigenvec_container).reshape(n, k)\n",
    "        \n",
    "        return(eigenvecs)\n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "        #Essentially I just call all the methods in the class PCA sequentially to fulfill their dependancies.\n",
    "        \n",
    "        this_transformed = self.standardize(X)\n",
    "        this_mean_vec = self.compute_mean_vector(this_transformed)\n",
    "        this_cov = self.compute_cov(this_transformed, this_mean_vec)\n",
    "        (these_eigenvals, these_eigenvecs) = self.compute_eigen_vector(this_cov)\n",
    "        these_eig_pairs = pca.make_eig_pairs(these_eigenvals, these_eigenvecs)\n",
    "        this_varexp = pca.compute_explained_variance(these_eigenvals)\n",
    "        this_cumexp = pca.cumulative_sum(this_varexp)\n",
    "        this_weight_matrix = pca.compute_weight_matrix(these_eig_pairs, this_cumexp)\n",
    "        \n",
    "        \n",
    "        print(len(this_weight_matrix),len(this_weight_matrix[0]))\n",
    "        return self.transform_data(X_std = this_transformed, matrix_w = this_weight_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d7503bc2e9d44a8932c77600c0b068c",
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[ PART A ]** Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b81e50a1e169900d57f6bde5059b554",
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa28b9a88044825ca1e35e05796e6a26",
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 191\n"
     ]
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
